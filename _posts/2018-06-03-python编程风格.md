---
title: python编程风格之好的代码逻辑
categories:
 - python
tags:
 - 编程风格
---
<blockquote>
  本文通过一个词频统计的代码，简单描述了python各种编程风格以及其特点，但风格无优劣，只是希望
希望读者能在项目中选择合适的编程风格。
</blockquote>     

## 引言
<blockquote>
  本文的介绍的项目很简单，给定一个文件，在去掉停用词之后，显示频率最高的N个词
</blockquote>

## 基本风格
### 1. 面条风格（瀑布风格）
特点：没有命名函数，没有对象，没有设计模式，拿起键盘直接干。
```python
#!/usr/bin/env python
import string

with open('../stop_words.txt') as f:
    stop_words = f.read().split(',')
stop_words.extend(list(string.ascii_lowercase))

word_count = {}
for line in open('../input.txt'):
    for word in line.split():
        if word not in stop_words:
            word_count.setdefault(word, 0)
            word_count[word] += 1

for tf in sorted(word_count.items(), key=lambda x: x[1], reverse=True):
    print tf[0], tf[1]
```
评价：从计算机诞生初期，该模式就已经兴起。写起来行云流水，一气呵成，外表看起来
波澜壮阔，内部有时却让人无从下手。虽然不被看好，但确延用至今，对于单一简单任务，
新手、老手都对它爱不释手。
### 2. 食谱风格
特点：使用过程抽象技术将一个大问题，拆解成若干小问题，并逐个解决，这些过程函数
使用全局变量的形式共享状态。
```python
#!/usr/bin/env python
# coding=utf-8
import string

data = []
word_freqs = []


def read_file(path_to_file):
    """
    读取文件单词
    :param path_to_file: 文件路径
    :return:
    """
    global data
    with open(path_to_file) as f:
        for word in f.readlines():
            data.extend(word.split(" "))


def filter_chars():
    """
    去掉空字符和换行符
    :return:
    """
    global data
    data = [d.strip() for d in data if d.strip()]


def remove_stop_words():
    """
    去掉停用词
    :return:
    """
    global data
    with open('../stop_words.txt') as f:
        stop_words = f.read().split(',')
    # add single-letter words
    stop_words.extend(list(string.ascii_lowercase))
    new_words = []
    for word in data:
        if word not in stop_words:
            new_words.append(word)
    data = new_words


def frequencies():
    """
    生成词频序列
    """
    global data
    global word_freqs
    word_count = {}
    for word in data:
        word_count.setdefault(word, 0)
        word_count[word] += 1
    word_freqs = word_count.items()


def sort():
    """
    词频排序
    """
    global word_freqs
    word_freqs.sort(lambda x, y: cmp(y[1], x[1]))


#
# The main function
#
read_file('../input.txt')
filter_chars()
remove_stop_words()
frequencies()
sort()

for tf in word_freqs[0:25]:
    print tf[0], ' - ', tf[1]
```
评价：每个过程函数独立处理一件事，结构化编程的第一步，通过数据共享为下一个过程函数提供数据，
就像是烹饪我们按照食谱进行每一步操作时，也会随着步骤的推进改变食材的状态。虽然数据共享会有一定的
风险，比如多放几次盐，食材的味道可能变的超出预期，但这就要取决利弊了。
### 3. 流水线风格（函数式编程）
特点：和食谱风格一样，但是这个函数间没有状态共享，函数根据输入形成输出。
```python
#!/usr/bin/env python
# coding=utf-8
import string


def read_file(path_to_file):
    """
    读取文件单词
    :param path_to_file: 文件路径
    :return []:
    """
    words = []
    with open(path_to_file) as f:
        for word in f.readlines():
            words.extend(word.split(" "))
    return words


def filter_chars(words):
    """
    去掉空字符和换行符
    :return:
    """
    return [d.strip() for d in words if d.strip()]


def remove_stop_words(words):
    """
    去掉停用词
    :return:
    """
    with open('../stop_words.txt') as f:
        stop_words = f.read().split(',')
    # add single-letter words
    stop_words.extend(list(string.ascii_lowercase))
    new_words = []
    for word in words:
        if word not in stop_words:
            new_words.append(word)
    return new_words


def frequencies(words):
    """
    生成词频序列
    """
    word_count = {}
    for word in words:
        word_count.setdefault(word, 0)
        word_count[word] += 1
    return word_count.items()


def sort(freq):
    """
    词频排序
    """
    freq.sort(lambda x, y: cmp(y[1], x[1]))
    return freq


#
# The main function
#
print(sort(frequencies(remove_stop_words(filter_chars(read_file('../input.txt'))))))
```
评价：和食谱风格既相似又不同，不同在于1、它不共享数据，2、(保持函数一致性)同函数多次调用输出结果都不变
它高度契合的数学的函数定义使它流传至今，如spark、mapreduce等等都对它进行了支持。
### 4. 极简风格
特点：多使用编程语言的高级特性和库来实现，让代码尽可能简洁
```python
#!/usr/bin/env python
import heapq, re, sys

words = re.findall("[a-z]{2,}", open('../input.txt').read().lower())
for w in heapq.nlargest(25, set(words) - set(open("../stop_words.txt").read().split(",")), words.count):
    print w, "-", words.count(w)
```
评价：运用得当，方可呼风唤雨，运用不当，小心走火入魔。
## 函数风格
### 1.递归风格
特点：使用数学归纳法将问题从0到n再到n+1推导
```python
#!/usr/bin/env python
import re, sys, operator


def count(word_list, stopwords, wordfreqs):
    if not word_list:
        return
    else:
        word = word_list[0]
        if word not in stopwords:
            if word in wordfreqs:
                wordfreqs[word] += 1
            else:
                wordfreqs[word] = 1
        count(word_list[1:], stopwords, wordfreqs)


def wf_print(wordfreq):
    if not wordfreq:
        return
    else:
        (w, c) = wordfreq[0]
        print w, '-', c
        wf_print(wordfreq[1:])


stop_words = set(open('../stop_words.txt').read().split(','))
words = re.findall('[a-z]{2,}', open('../input.txt').read().lower())
word_freqs = {}
CHUNK = 9500
for i in range(0, len(words), CHUNK):
    count(words[i:i+CHUNK], stop_words, word_freqs)

wf_print(sorted(word_freqs.iteritems(), key=operator.itemgetter(1), reverse=True)[:25])
```
评论：递归一时爽，堆栈火葬场，要想用的爽就需要有效减少递归的层数，或者像以上代码一样将数据
分块进行递归处理
### 2.callback风格
特点：每个函数都有一个额外的函数参数，并且在函数结束时调用该参数
```python
#!/usr/bin/env python
import sys, re, operator, string


def read_file(path_to_file, func):
    with open(path_to_file) as f:
        data = f.read()
    func(data, normalize)


def filter_chars(str_data, func):
    pattern = re.compile('[\W_]+')
    func(pattern.sub(' ', str_data), scan)


def normalize(str_data, func):
    func(str_data.lower(), remove_stop_words)


def scan(str_data, func):
    func(str_data.split(), frequencies)


def remove_stop_words(word_list, func):
    with open('../stop_words.txt') as f:
        stop_words = f.read().split(',')
    # add single-letter words
    stop_words.extend(list(string.ascii_lowercase))
    func([w for w in word_list if not w in stop_words], sort)


def frequencies(word_list, func):
    wf = {}
    for w in word_list:
        if w in wf:
            wf[w] += 1
        else:
            wf[w] = 1
    func(wf, print_text)


def sort(wf, func):
    func(sorted(wf.iteritems(), key=operator.itemgetter(1), reverse=True), no_op)


def print_text(word_freqs, func):
    for (w, c) in word_freqs[0:25]:
        print w, "-", c
    func(None)


def no_op(func):
    return


read_file('../input.txt', filter_chars)
```
评价：常用于真对程序的同状态执行不同操作，异步必会写法。但是你凝望着深渊，深渊也在凝望你，小心回调地狱。
### 3.真*流式风格
特点：需要一个可以修改数据的类，并将流水线风格的方法绑定在一起。
```python
#!/usr/bin/env python
import sys, re, operator, string


class TFTheOne:
    def __init__(self, v):
        self._value = v

    def bind(self, func):
        self._value = func(self._value)
        return self

    def printme(self):
        print self._value


def read_file(path_to_file):
    with open(path_to_file) as f:
        data = f.read()
    return data


def filter_chars(str_data):
    pattern = re.compile('[\W_]+')
    return pattern.sub(' ', str_data)


def normalize(str_data):
    return str_data.lower()


def scan(str_data):
    return str_data.split()


def remove_stop_words(word_list):
    with open('../stop_words.txt') as f:
        stop_words = f.read().split(',')
    # add single-letter words
    stop_words.extend(list(string.ascii_lowercase))
    return [w for w in word_list if not w in stop_words]


def frequencies(word_list):
    word_freqs = {}
    for w in word_list:
        if w in word_freqs:
            word_freqs[w] += 1
        else:
            word_freqs[w] = 1
    return word_freqs


def sort(word_freq):
    return sorted(word_freq.iteritems(), key=operator.itemgetter(1), reverse=True)


def top25_freqs(word_freqs):
    top25 = ""
    for tf in word_freqs[0:25]:
        top25 += str(tf[0]) + ' - ' + str(tf[1]) + '\n'
    return top25


TFTheOne(sys.argv[1]) \
    .bind(read_file) \
    .bind(filter_chars) \
    .bind(normalize) \
    .bind(scan) \
    .bind(remove_stop_words) \
    .bind(frequencies) \
    .bind(sort) \
    .bind(top25_freqs) \
    .printme()
```
评价：绑定方法返回自身，并且封装操作隔离了全局变量，绑定操作实现了隐式数据传递，
最后的拆分打开了封装。
## 面向对象风格
### 1.对象风格
### 2.消息风格
### 3.闭域风格
### 4.抽象对象风格
### 5.好莱坞风格
### 6.公告板风格
## 反射与元编程
## 异常处理
## 项目类型主导的基本风格
#### 以数据为中心
## 并发
### 1.参与者风格
特点：将大问题分解为问题相关对象，并每个对象都有一个消息队列，对象仅公开接口。
```python
#!/usr/bin/env python

import sys, re, operator, string
from threading import Thread
from Queue import Queue


class ActiveWFObject(Thread):
    def __init__(self):
        Thread.__init__(self)
        self.name = str(type(self))
        self.queue = Queue()
        self._stop = False
        self.start()

    def run(self):
        while not self._stop:
            message = self.queue.get()
            self._dispatch(message)
            if message[0] == 'die':
                self._stop = True

def send(receiver, message):
    receiver.queue.put(message)

class DataStorageManager(ActiveWFObject):
    """ Models the contents of the file """
    _data = ''

    def _dispatch(self, message):
        if message[0] == 'init':
            self._init(message[1:])
        elif message[0] == 'send_word_freqs':
            self._process_words(message[1:])
        else:
            # forward
            send(self._stop_word_manager, message)
 
    def _init(self, message):
        path_to_file = message[0]
        self._stop_word_manager =    [1]
        with open(path_to_file) as f:
            self._data = f.read()
        pattern = re.compile('[\W_]+')
        self._data = pattern.sub(' ', self._data).lower()

    def _process_words(self, message):
        recipient = message[0]
        data_str = ''.join(self._data)
        words = data_str.split()
        for w in words:
            send(self._stop_word_manager, ['filter', w])
        send(self._stop_word_manager, ['top25', recipient])

class StopWordManager(ActiveWFObject):
    """ Models the stop word filter """
    _stop_words = []

    def _dispatch(self, message):
        if message[0] == 'init':
            self._init(message[1:])
        elif message[0] == 'filter':
            return self._filter(message[1:])
        else:
            # forward
            send(self._word_freqs_manager, message)
 
    def _init(self, message):
        with open('../stop_words.txt') as f:
            self._stop_words = f.read().split(',')
        self._stop_words.extend(list(string.ascii_lowercase))
        self._word_freqs_manager = message[0]

    def _filter(self, message):
        word = message[0]
        if word not in self._stop_words:
            send(self._word_freqs_manager, ['word', word])

class WordFrequencyManager(ActiveWFObject):
    """ Keeps the word frequency data """
    _word_freqs = {}

    def _dispatch(self, message):
        if message[0] == 'word':
            self._increment_count(message[1:])
        elif message[0] == 'top25':
            self._top25(message[1:])
 
    def _increment_count(self, message):
        word = message[0]
        if word in self._word_freqs:
            self._word_freqs[word] += 1 
        else: 
            self._word_freqs[word] = 1

    def _top25(self, message):
        recipient = message[0]
        freqs_sorted = sorted(self._word_freqs.iteritems(), key=operator.itemgetter(1), reverse=True)
        send(recipient, ['top25', freqs_sorted])

class WordFrequencyController(ActiveWFObject):

    def _dispatch(self, message):
        if message[0] == 'run':
            self._run(message[1:])
        elif message[0] == 'top25':
            self._display(message[1:])
        else:
            raise Exception("Message not understood " + message[0])
 
    def _run(self, message):
        self._storage_manager = message[0]
        send(self._storage_manager, ['send_word_freqs', self])

    def _display(self, message):
        word_freqs = message[0]
        for (w, f) in word_freqs[0:25]:
            print w, ' - ', f
        send(self._storage_manager, ['die'])
        self._stop = True

#
# The main function
#
word_freq_manager = WordFrequencyManager()

stop_word_manager = StopWordManager()
send(stop_word_manager, ['init', word_freq_manager])

storage_manager = DataStorageManager()
send(storage_manager, ['init', "../input.txt", stop_word_manager])

wfcontroller = WordFrequencyController()
send(wfcontroller, ['run', storage_manager])

# Wait for the active objects to finish
[t.join() for t in [word_freq_manager, stop_word_manager, storage_manager, wfcontroller]]
```
评价：模块拆解，通讯独立，各个模块能相互配合，分布式模型之一。但在通信任务较多或者模块配合复杂式通信程序编写复杂不易维护。
### 2.数据空间风格
特点：按照数据流的类型定义数据空间，再根据数据空间定义相应的worker
```python
#!/usr/bin/env python
import re, sys, operator, Queue, threading

# 两个数据空间
word_space = Queue.Queue()
freq_space = Queue.Queue()

stopwords = set(open('../stop_words.txt').read().split(','))


def process_words():
    word_freqs = {}
    while True:
        try:
            word = word_space.get(timeout=1)
        except Queue.Empty:
            break

        if word not in stopwords:
            word_freqs.setdefault(word, 0)
            word_freqs[word] += 1
    freq_space.put(word_freqs)


for word in re.findall('[a-z]{2,}', open("../input.txt", 'r').read().lower()):
    word_space.put(word)

workers = []
for i in range(5):
    workers.append(threading.Thread(target=process_words))
[t.start() for t in workers]
[t.join() for t in workers]

word_freqs = {}
while not freq_space.empty():
    freqs = freq_space.get()
    print(freqs)
    for (k, v) in freqs.iteritems():
        if k in word_freqs:
            count = sum(item[k] for item in [freqs, word_freqs])
        else:
            count = freqs[k]
        word_freqs[k] = count

for (w, c) in sorted(word_freqs.iteritems(), key=operator.itemgetter(1), reverse=True)[:25]:
    print w, '-', c
```
评价：对于数据密集型，该风格得心应手，但对于并发需要相互配合的状态，则无能为力。
### 3.Map Reduce风格
特点：map对每个数据块运用定义函数，reduce对map的结果进行整合
```python
#!/usr/bin/env python
import sys, re, operator, string

def partition(data_str, nlines):
    """ 
    Partitions the input data_str (a big string)
    into chunks of nlines.
    """
    lines = data_str.split('\n')
    for i in xrange(0, len(lines), nlines):
        yield '\n'.join(lines[i:i+nlines])

def split_words(data_str):
    """ 
    Takes a string,  returns a list of pairs (word, 1), 
    one for each word in the input, so
    [(w1, 1), (w2, 1), ..., (wn, 1)]
    """
    def _scan(str_data):
        pattern = re.compile('[\W_]+')
        return pattern.sub(' ', str_data).lower().split()

    def _remove_stop_words(word_list):
        with open('../stop_words.txt') as f:
            stop_words = f.read().split(',')
        stop_words.extend(list(string.ascii_lowercase))
        return [w for w in word_list if not w in stop_words]

    # The actual work of splitting the input into words
    result = []
    words = _remove_stop_words(_scan(data_str))
    for w in words:
        result.append((w, 1))
    return result

def count_words(pairs_list_1, pairs_list_2):
    """ 
    Takes two lists of pairs of the form
    [(w1, 1), ...]
    and returns a list of pairs [(w1, frequency), ...], 
    where frequency is the sum of all the reported occurrences
    """
    mapping = dict((k, v) for k, v in pairs_list_1)
    for p in pairs_list_2:
        mapping.setdefault(p[0], 0)
        mapping[p[0]] += p[1]
    return mapping.items()

def read_file(path_to_file):
    with open(path_to_file) as f:
        data = f.read()
    return data

def sort(word_freq):
    return sorted(word_freq, key=operator.itemgetter(1), reverse=True)

splits = map(split_words, partition(read_file("../input.txt"), 200))
splits.insert(0, []) # Normalize input to reduce
print()
word_freqs = sort(reduce(count_words, splits))

for (w, c) in word_freqs[0:25]:
    print w, ' - ', c
```
特点：分治合并，对于超大数据量威力无穷。
### 4.双重Map Reduce风格
特点：同上
```python
#!/usr/bin/env python
import sys, re, operator, string

def partition(data_str, nlines):
    """ 
    Partitions the input data_str (a big string)
    into chunks of nlines.
    """
    lines = data_str.split('\n')
    for i in xrange(0, len(lines), nlines):
        yield '\n'.join(lines[i:i+nlines])

def split_words(data_str):
    """ 
    Takes a string, returns a list of pairs (word, 1), 
    one for each word in the input, so
    [(w1, 1), (w2, 1), ..., (wn, 1)]
    """
    def _scan(str_data):
        pattern = re.compile('[\W_]+')
        return pattern.sub(' ', str_data).lower().split()

    def _remove_stop_words(word_list):
        with open('../stop_words.txt') as f:
            stop_words = f.read().split(',')
        stop_words.extend(list(string.ascii_lowercase))
        return [w for w in word_list if not w in stop_words]

    result = []
    words = _remove_stop_words(_scan(data_str))
    for w in words:
        result.append((w, 1))
    return result

def regroup(pairs_list):
    """
    Takes a list of lists of pairs of the form 
    [[(w1, 1), (w2, 1), ..., (wn, 1)],
     [(w1, 1), (w2, 1), ..., (wn, 1)],
     ...]
    and returns a dictionary mapping each unique word to the 
    corresponding list of pairs, so
    { w1 : [(w1, 1), (w1, 1)...], 
      w2 : [(w2, 1), (w2, 1)...], 
      ...}
    """
    mapping = {}
    for pairs in pairs_list:
        for p in pairs:
            mapping.setdefault(p[0], [])
            mapping[p[0]].append(p)
    return mapping
    
def count_words(mapping):
    """ 
    Takes a mapping of the form (word, [(word, 1), (word, 1)...)])
    and returns a pair (word, frequency), where frequency is the 
    sum of all the reported occurrences
    """
    def add(x, y):
        return x+y

    return (mapping[0], reduce(add, (pair[1] for pair in mapping[1])))

def read_file(path_to_file):
    with open(path_to_file) as f:
        data = f.read()
    return data

def sort(word_freq):
    return sorted(word_freq, key=operator.itemgetter(1), reverse=True)

splits = map(split_words, partition(read_file("../input.txt"), 200))
splits_per_word = regroup(splits)
word_freqs = sort(map(count_words, splits_per_word.items()))

for (w, c) in word_freqs[0:25]:
    print w, ' - ', c
```
特点：解决单一mapreduce时reduce无法并行计算的问题
#### 交互